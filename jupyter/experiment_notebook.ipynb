{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting started\n",
    "\n",
    "First, we enable the cluster to scale up. Note that if you run an auto-scaling cluster,\n",
    "Google will suspend your nodes. Make sure to have the experiment prepared before running the commands.\n",
    "\n",
    "The following is assumed ready:\n",
    "* GKE/Kubernetes cluster (see also `terraform/terraform_notebook.ipynb`)\n",
    "    * 2 nodes pools (default for system & dependencies, experiment pool)\n",
    "* Docker image (including dataset, to speed-up starting experiments).\n",
    "    * Within a bash shell\n",
    "        * Make sure to have the `requirements-cpu.txt` installed (or `requirements-gpu.txt (in a virtual venv/conda environment). You can run `pip3 install -r requirements-cpu.txt`\n",
    "    * First run the extractor (locally) `python3 -m fltk extractor configs/example_cloud_experiment.json`\n",
    "        *  This downloads datasets to be included in the docker image.\n",
    "    * Build the container `DOCKER_BUILDKIT=1 docker build --platform linux/amd64 . --tag gcr.io/$PROJECT_ID/fltk`\n",
    "    * Push to your gcr.io repository `docker push gcr.io/$PROJECT_ID/fltk`\n",
    "\n",
    "\n",
    "With that setup, first set some variables used throughout the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for fltk-testbed-cluster.\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "### CHANGE ME! ###\n",
    "##################\n",
    "PROJECT_ID=\"fltk-group-11\"\n",
    "CLUSTER_NAME=\"fltk-testbed-cluster\"\n",
    "DEFAULT_POOL=\"default-node-pool\"\n",
    "EXPERIMENT_POOL=\"medium-fltk-pool-1\"\n",
    "REGION=\"us-central1-c\"\n",
    "\n",
    "# In case we do not yet have the credentials/kubeconfig\n",
    "gcloud container clusters get-credentials $CLUSTER_NAME --region $REGION --project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Scale the default-node-pool up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing fltk-testbed-cluster...done.                                          \n",
      "Updated [https://container.googleapis.com/v1/projects/fltk-group-11/zones/us-central1-c/clusters/fltk-testbed-cluster].\n",
      "Resizing fltk-testbed-cluster...done.                                          \n",
      "Updated [https://container.googleapis.com/v1/projects/fltk-group-11/zones/us-central1-c/clusters/fltk-testbed-cluster].\n"
     ]
    }
   ],
   "source": [
    "# These commands might take a while to complete.\n",
    "gcloud container clusters resize $CLUSTER_NAME --node-pool $DEFAULT_POOL \\\n",
    "     --num-nodes 2 --region $REGION --quiet\n",
    "\n",
    "gcloud container clusters resize $CLUSTER_NAME --node-pool $EXPERIMENT_POOL \\\n",
    "    --num-nodes 2 --region $REGION --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation\n",
    "In case you have already tested something or ran another experiment, we have to remove the deployment of the Orchestrator. This will not delete any experiment data, as this persists on one of the ReadWriteMany PVCs.\n",
    "\n",
    "\n",
    "Currently, the Orchestrator is deployed using a `Deployment` definition, a future version will replace this with a `Deployment` definition, to make this step unnecessary. For experiments this means the following:\n",
    "\n",
    "1. A single deployment can exist at a single time in a single namespace. This includes 'completed' experiments.\n",
    "2. For running batches of experiments, a BatchOrchestrator is provided.\n",
    "\n",
    "\n",
    "ℹ️ This will not remove any data, but if your orchestrator is still/already running experiments, this will stop the deployment. Running training jobs will not be stopped, for this you can use `kubectl`. ConfigMaps created by the Orchestrator (to provide experiment configurations), will not be removed. See the commented code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: uninstall: Release not loaded: experiment-extractor: release: not found\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# If you want to delete all pytorch trainjobs, uncomment the command below.\n",
    "# kubectl delete pytorchjobs.kubeflow.org --all --namespace test\n",
    "\n",
    "# If you want to delete all existing configuration map objects in a namespace, run teh command below\n",
    "# kubectl delete configmaps --all --namespace test\n",
    "\n",
    "helm uninstall -n test experiment-extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install extractor\n",
    "\n",
    "Deploy the TensorBoard service and persistent volumes, required for deployment of the orchestrator's chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      \tNAMESPACE\tREVISION\tUPDATED                                 \tSTATUS  \tCHART                       \tAPP VERSION\n",
      "nfs-server\ttest     \t1       \t2022-09-22 17:21:31.971502719 +0200 CEST\tdeployed\tnfs-server-provisioner-1.1.3\t2.3.0      \n"
     ]
    }
   ],
   "source": [
    "helm ls -n test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"experiment-extractor\" does not exist. Installing it now.\n",
      "NAME: experiment-extractor\n",
      "LAST DEPLOYED: Thu Oct  6 20:35:36 2022\n",
      "NAMESPACE: test\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "Get the FLTK extractors Tensorboard URL by running:\n",
      "\n",
      "export POD_NAME=$(kubectl get pods -n test -l \"app.kubernetes.io/name=fltk.extractor\" -o jsonpath=\"{.items[0].metadata.name}\")\n",
      "echo http://localhost:6006/\n",
      "kubectl -n test port-forward $POD_NAME 6006:6006\n"
     ]
    }
   ],
   "source": [
    "helm upgrade --install -n test experiment-extractor ../charts/extractor -f ../charts/fltk-values.yaml \\\n",
    "    --set provider.projectName=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:6006/\n",
      "Forwarding from [::1]:6006 -> 6006\n",
      "E1005 16:59:42.763253   70349 portforward.go:233] lost connection to pod\n"
     ]
    }
   ],
   "source": [
    "export POD_NAME=$(kubectl get pods -n test -l \"app.kubernetes.io/name=fltk.extractor\" -o jsonpath=\"{.items[0].metadata.name}\")\n",
    "echo http://localhost:6006/\n",
    "kubectl -n test port-forward $POD_NAME 6006:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define experiment configuration files\n",
    "\n",
    "Deployment of experiments is currently done through a Helm Deployment. A future release (™️) will rework this to a Job definition, as this allows to re-use the template more easily.\n",
    "\n",
    "\n",
    "> The `EXPERIMENT_FILE` will contain the description of the experiments\n",
    "> The `CLUSTER_CONFIG` will contain shared configurations for logging, Orchestrator configuration and replication information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_FILE=\"../configs/distributed_tasks/example_arrival_config.json\"\n",
    "CLUSTER_CONFIG=\"../configs/example_cloud_experiment.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup experiment variables\n",
    "Next, we will deploy the experiments.\n",
    "\n",
    "\n",
    "We provide a configuration file, `charts/fltk-values.yaml`, in here change the values under the `provider` block. Change `projectName` to your Google Cloud Project ID.\n",
    "\n",
    "```yaml\n",
    "provider:\n",
    "    domain: gcr.io\n",
    "    projectName: CHANGE_ME!\n",
    "    imageName: fltk:latest\n",
    "```\n",
    "\n",
    "We use the `--set-file` flag for `helm`, as currently, Helm does not support using files outside of the chart root directory (in this case `charts/orchestrator`). Using `--set-file` we can dynamically provide these files. See also issue [here](https://github.com/helm/helm/issues/3276)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"experiment-orchestrator\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "helm uninstall -n test experiment-orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: experiment-orchestrator\n",
      "LAST DEPLOYED: Thu Oct  6 20:35:53 2022\n",
      "NAMESPACE: test\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "You successfully launched an experiment configuration on your cluster in test.\n",
      "\n",
      "N.B. Make sure to collect all data after completing your experiment!\n",
      "N.B. Re-installing the orchestrator WILL RESULT IN DELETION OF ALL TRAINJOBS and PODS!\n"
     ]
    }
   ],
   "source": [
    "helm install -n test experiment-orchestrator ../charts/orchestrator -f ../charts/fltk-values.yaml \\\n",
    "    --set-file orchestrator.experiment=$EXPERIMENT_FILE,orchestrator.configuration=$CLUSTER_CONFIG \\\n",
    "    --set provider.projectName=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                     READY   STATUS      RESTARTS   AGE\n",
      "fl-extractor-5d555cdb84-fjq9b                            1/1     Running     0          27m\n",
      "fl-server                                                0/1     Completed   0          27m\n",
      "nfs-server-nfs-server-provisioner-0                      1/1     Running     0          160m\n",
      "trainjob-5942b8f9-c727-4b7c-a897-54eb15e9b56a-master-0   0/1     Completed   0          23m\n",
      "trainjob-5942b8f9-c727-4b7c-a897-54eb15e9b56a-worker-0   0/1     Completed   0          23m\n",
      "trainjob-79fd6aa7-8be2-4df1-8be6-78af665f4fc8-master-0   0/1     Completed   0          23m\n",
      "trainjob-79fd6aa7-8be2-4df1-8be6-78af665f4fc8-worker-0   0/1     Completed   0          23m\n",
      "trainjob-7cfef02f-fb1e-4283-8792-102764db7137-master-0   0/1     Completed   0          23m\n",
      "trainjob-7cfef02f-fb1e-4283-8792-102764db7137-worker-0   0/1     Completed   0          23m\n",
      "trainjob-8b94bb13-7af9-44c2-b11b-07b94486f507-master-0   0/1     Completed   0          23m\n",
      "trainjob-8b94bb13-7af9-44c2-b11b-07b94486f507-worker-0   0/1     Completed   0          23m\n",
      "trainjob-dd171e67-ccc5-4434-a814-0c4280a1f96e-master-0   0/1     Completed   0          25m\n",
      "trainjob-dd171e67-ccc5-4434-a814-0c4280a1f96e-worker-0   0/1     Completed   0          25m\n"
     ]
    }
   ],
   "source": [
    "kubectl get pods -n test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# To get logs from the orchestrator\n",
    "kubectl logs -n test fl-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-06-2022 18:40:18 root         INFO     Loading file config/configuration.fltk.json\n",
      "10-06-2022 18:40:18 root         INFO     Starting in client mode\n",
      "10-06-2022 18:40:18 root         INFO     Starting with host=trainjob-79fd6aa7-8be2-4df1-8be6-78af665f4fc8-master-0 and port=23456\n",
      "10-06-2022 18:40:18 root         INFO     Initializing backend for training process: gloo\n",
      "10-06-2022 18:40:22 torch.distributed.distributed_c10d INFO     Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "10-06-2022 18:40:22 torch.distributed.distributed_c10d INFO     Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "10-06-2022 18:40:22 root         INFO     Starting Creating client with 0\n",
      "10-06-2022 18:40:22 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     Initializing learning client\n",
      "10-06-2022 18:40:23 root         INFO     Getting net: Nets.fashion_mnist_cnn\n",
      "10-06-2022 18:40:23 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     Preparing learner model with distributed=True\n",
      "10-06-2022 18:40:24 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [1,     0] loss: 0.047\n",
      "10-06-2022 18:40:24 torch.nn.parallel.distributed INFO     Reducer buckets have been rebuilt in this iteration.\n",
      "10-06-2022 18:41:11 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [1,    50] loss: 0.743\n",
      "10-06-2022 18:42:02 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [1,   100] loss: 0.471\n",
      "10-06-2022 18:42:49 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [1,   150] loss: 0.414\n",
      "10-06-2022 18:43:35 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [1,   200] loss: 0.387\n",
      "10-06-2022 18:44:27 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [2,     0] loss: 0.009\n",
      "10-06-2022 18:45:19 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [2,    50] loss: 0.349\n",
      "10-06-2022 18:46:10 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [2,   100] loss: 0.327\n",
      "10-06-2022 18:46:58 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [2,   150] loss: 0.318\n",
      "10-06-2022 18:47:47 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [2,   200] loss: 0.307\n",
      "10-06-2022 18:48:42 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [3,     0] loss: 0.008\n",
      "10-06-2022 18:49:36 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [3,    50] loss: 0.298\n",
      "10-06-2022 18:50:23 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [3,   100] loss: 0.281\n",
      "10-06-2022 18:51:16 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [3,   150] loss: 0.277\n",
      "10-06-2022 18:52:04 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [3,   200] loss: 0.270\n",
      "10-06-2022 18:53:03 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [4,     0] loss: 0.007\n",
      "10-06-2022 18:53:53 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [4,    50] loss: 0.268\n",
      "10-06-2022 18:54:44 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [4,   100] loss: 0.252\n",
      "10-06-2022 18:55:35 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [4,   150] loss: 0.249\n",
      "10-06-2022 18:56:31 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [4,   200] loss: 0.244\n",
      "10-06-2022 18:57:30 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [5,     0] loss: 0.006\n",
      "10-06-2022 18:58:25 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [5,    50] loss: 0.245\n",
      "10-06-2022 18:59:18 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [5,   100] loss: 0.231\n",
      "10-06-2022 19:00:16 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [5,   150] loss: 0.226\n",
      "10-06-2022 19:01:12 Client-0-79fd6aa7-8be2-4df1-8be6-78af665f4fc8 INFO     [5,   200] loss: 0.223\n",
      "10-06-2022 19:02:06 root         INFO     Stopping client...\n",
      "No argument path is provided.\n",
      "[EpochData(epoch_id=1, num_epochs=6, duration_train=223794, duration_test=20079, loss_train=0.3866809019446373, accuracy=84.66, loss=16.361688256263733, cpu_usage=29.3, class_precision=array([0.85746102, 0.974     , 0.87601078, 0.89221557, 0.8021978 ,\n",
      "       0.95869565, 0.50333778, 0.93842887, 0.94230769, 0.90458015]), class_recall=array([0.74902724, 0.97791165, 0.64612326, 0.85632184, 0.70736434,\n",
      "       0.94635193, 0.76938776, 0.8875502 , 0.96646943, 0.97530864]), confusion_mat=array([[385,   0,   2,  17,   3,   3,  97,   0,   7,   0],\n",
      "       [  1, 487,   0,   4,   0,   0,   3,   0,   3,   0],\n",
      "       [  4,   1, 325,   3,  52,   0, 115,   0,   3,   0],\n",
      "       [  7,   8,   2, 447,  11,   0,  43,   0,   4,   0],\n",
      "       [  1,   1,  22,  22, 365,   0, 104,   0,   1,   0],\n",
      "       [  0,   0,   0,   0,   0, 441,   1,  18,   0,   6],\n",
      "       [ 50,   2,  20,   6,  23,   0, 377,   0,  12,   0],\n",
      "       [  0,   0,   0,   0,   0,  13,   0, 442,   0,  43],\n",
      "       [  1,   1,   0,   2,   1,   1,   8,   2, 490,   1],\n",
      "       [  0,   0,   0,   0,   0,   2,   1,   9,   0, 474]])), EpochData(epoch_id=2, num_epochs=6, duration_train=478072, duration_test=18316, loss_train=0.3074443006515503, accuracy=86.06, loss=15.128326028585434, cpu_usage=29.7, class_precision=array([0.88940092, 0.9702381 , 0.87212276, 0.87547893, 0.82608696,\n",
      "       0.98190045, 0.53314917, 0.92629482, 0.95890411, 0.93137255]), class_recall=array([0.75097276, 0.98192771, 0.67793241, 0.87547893, 0.73643411,\n",
      "       0.93133047, 0.7877551 , 0.93373494, 0.96646943, 0.97736626]), confusion_mat=array([[386,   0,   3,  17,   1,   0, 100,   1,   6,   0],\n",
      "       [  1, 489,   0,   3,   0,   0,   3,   0,   2,   0],\n",
      "       [  3,   1, 341,   5,  46,   0, 104,   0,   3,   0],\n",
      "       [  6,   8,   2, 457,  11,   0,  36,   0,   2,   0],\n",
      "       [  1,   2,  20,  28, 380,   0,  85,   0,   0,   0],\n",
      "       [  0,   0,   0,   0,   0, 434,   1,  24,   0,   7],\n",
      "       [ 37,   3,  25,  10,  21,   0, 386,   0,   8,   0],\n",
      "       [  0,   0,   0,   0,   0,   6,   0, 465,   0,  27],\n",
      "       [  0,   1,   0,   2,   1,   1,   9,   2, 490,   1],\n",
      "       [  0,   0,   0,   0,   0,   1,   0,  10,   0, 475]])), EpochData(epoch_id=3, num_epochs=6, duration_train=738691, duration_test=19791, loss_train=0.2697051912546158, accuracy=86.84, loss=14.744443356990814, cpu_usage=29.7, class_precision=array([0.89195402, 0.95348837, 0.88349515, 0.89641434, 0.83509514,\n",
      "       0.98630137, 0.55397727, 0.92490119, 0.96640316, 0.93307087]), class_recall=array([0.75486381, 0.98795181, 0.72365805, 0.86206897, 0.76550388,\n",
      "       0.92703863, 0.79591837, 0.93975904, 0.96449704, 0.97530864]), confusion_mat=array([[388,   3,   2,  14,   1,   0, 100,   0,   6,   0],\n",
      "       [  0, 492,   0,   2,   0,   0,   2,   0,   2,   0],\n",
      "       [  4,   3, 364,   2,  39,   0,  90,   0,   1,   0],\n",
      "       [  6,  12,   2, 450,  14,   0,  36,   0,   2,   0],\n",
      "       [  1,   2,  22,  22, 395,   0,  74,   0,   0,   0],\n",
      "       [  0,   0,   0,   0,   0, 432,   1,  26,   0,   7],\n",
      "       [ 36,   3,  22,  10,  23,   0, 390,   0,   6,   0],\n",
      "       [  0,   0,   0,   0,   0,   4,   0, 468,   0,  26],\n",
      "       [  0,   1,   0,   2,   1,   1,  10,   2, 489,   1],\n",
      "       [  0,   0,   0,   0,   0,   1,   1,  10,   0, 474]])), EpochData(epoch_id=4, num_epochs=6, duration_train=1006180, duration_test=19293, loss_train=0.24394446283578872, accuracy=87.58, loss=14.086983874440193, cpu_usage=29.9, class_precision=array([0.88392857, 0.9391635 , 0.87045455, 0.91823899, 0.82186235,\n",
      "       0.98430493, 0.58787879, 0.93478261, 0.972167  , 0.946     ]), class_recall=array([0.77042802, 0.99196787, 0.76143141, 0.83908046, 0.78682171,\n",
      "       0.94206009, 0.79183673, 0.9497992 , 0.96449704, 0.97325103]), confusion_mat=array([[396,   4,   3,  10,   1,   0,  94,   0,   6,   0],\n",
      "       [  0, 494,   0,   1,   0,   0,   1,   0,   2,   0],\n",
      "       [  4,   3, 383,   2,  41,   0,  69,   0,   1,   0],\n",
      "       [  8,  17,   4, 438,  20,   0,  34,   0,   1,   0],\n",
      "       [  1,   3,  27,  17, 406,   0,  62,   0,   0,   0],\n",
      "       [  0,   0,   0,   0,   0, 439,   1,  20,   0,   6],\n",
      "       [ 39,   4,  23,   7,  25,   0, 388,   0,   4,   0],\n",
      "       [  0,   0,   0,   0,   0,   5,   0, 473,   0,  20],\n",
      "       [  0,   1,   0,   2,   1,   1,  10,   2, 489,   1],\n",
      "       [  0,   0,   0,   0,   0,   1,   1,  11,   0, 473]])), EpochData(epoch_id=5, num_epochs=6, duration_train=1286392, duration_test=16475, loss_train=0.2231755793094635, accuracy=87.96, loss=13.505268424749374, cpu_usage=30.5, class_precision=array([0.88741722, 0.9391635 , 0.875     , 0.92356688, 0.82765531,\n",
      "       0.97587719, 0.59360731, 0.93503937, 0.978     , 0.95918367]), class_recall=array([0.78210117, 0.99196787, 0.76540755, 0.83333333, 0.8003876 ,\n",
      "       0.95493562, 0.79591837, 0.95381526, 0.96449704, 0.96707819]), confusion_mat=array([[402,   5,   3,  10,   2,   0,  88,   0,   4,   0],\n",
      "       [  0, 494,   0,   1,   0,   0,   1,   0,   2,   0],\n",
      "       [  5,   1, 385,   2,  37,   0,  73,   0,   0,   0],\n",
      "       [  8,  19,   5, 435,  22,   0,  32,   0,   1,   0],\n",
      "       [  1,   2,  23,  14, 413,   0,  63,   0,   0,   0],\n",
      "       [  0,   0,   0,   0,   0, 445,   0,  17,   0,   4],\n",
      "       [ 37,   4,  24,   7,  24,   0, 390,   0,   4,   0],\n",
      "       [  0,   0,   0,   0,   0,   7,   0, 475,   0,  16],\n",
      "       [  0,   1,   0,   2,   1,   2,  10,   2, 489,   0],\n",
      "       [  0,   0,   0,   0,   0,   2,   0,  14,   0, 470]]))]\n"
     ]
    }
   ],
   "source": [
    "# To get logs from learners (example)\n",
    "kubectl logs -n test trainjob-79fd6aa7-8be2-4df1-8be6-78af665f4fc8-master-0   \n",
    "# To get logs from learners (federated learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy experiment results from the extractor\n",
    "\n",
    "Extractor holds the experiment results in the format that can be processedby TensorBoard.\n",
    "In order to download it to the local machine, execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n"
     ]
    }
   ],
   "source": [
    "EXTRACTOR_POD_NAME=$(kubectl get pods -n test -l \"app.kubernetes.io/name=fltk.extractor\" -o jsonpath=\"{.items[0].metadata.name}\")\n",
    "\n",
    "kubectl cp -n test $EXTRACTOR_POD_NAME:/opt/federation-lab/logging ./logging_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"experiment-orchestrator\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "helm uninstall -n test experiment-orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing extractor\n",
    "\n",
    "IMPORTANT: Removing extractor chart will result in deleting the already collected experiment results, stored in the NFS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"experiment-extractor\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "helm uninstall experiment-extractor -n test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wrapping up\n",
    "\n",
    "To scale down the cluster nodepools, run the cell below. This will scale the node pools down and remove all the experiments deployed (on the cluster).\n",
    "\n",
    "1. Experiments cannot be restarted.\n",
    "2. Experiment logs will not persist deletion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorchjob.kubeflow.org \"trainjob-3d330c98-29f9-4cc7-a335-2aa669b8bf5b\" deleted\n",
      "pytorchjob.kubeflow.org \"trainjob-63353c95-3ebd-406f-beeb-d743af5fc79b\" deleted\n",
      "pytorchjob.kubeflow.org \"trainjob-6a851cd6-ff87-4aec-9496-41253d9900b8\" deleted\n",
      "pytorchjob.kubeflow.org \"trainjob-6f739053-97e9-46b9-a139-3d01e79c09e5\" deleted\n",
      "pytorchjob.kubeflow.org \"trainjob-7496757f-227e-43f6-b000-e7b597f624ea\" deleted\n",
      "pytorchjob.kubeflow.org \"trainjob-9361188a-9990-416d-9e4b-862e5b5496f3\" deleted\n"
     ]
    }
   ],
   "source": [
    "kubectl delete pytorchjobs.kubeflow.org --all-namespaces --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing fltk-testbed-cluster...done.                                          \n",
      "Updated [https://container.googleapis.com/v1/projects/fltk-group-11/zones/us-central1-c/clusters/fltk-testbed-cluster].\n",
      "Resizing fltk-testbed-cluster...done.                                          \n",
      "Updated [https://container.googleapis.com/v1/projects/fltk-group-11/zones/us-central1-c/clusters/fltk-testbed-cluster].\n"
     ]
    }
   ],
   "source": [
    "# This will remove all information and logs as well.\n",
    "\n",
    "gcloud container clusters resize $CLUSTER_NAME --node-pool $DEFAULT_POOL \\\n",
    "    --num-nodes 0 --region $REGION --quiet\n",
    "\n",
    "gcloud container clusters resize $CLUSTER_NAME --node-pool $EXPERIMENT_POOL \\\n",
    "    --num-nodes 0 --region $REGION --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "title": "Experiment deployment"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
